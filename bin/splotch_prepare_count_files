#!/usr/bin/env python

from __future__ import absolute_import, division, print_function

import argparse
import logging
import os
import sys

import numpy
import pandas as pd

import splotch

logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)

def prepare_count_files(count_files,suffix='.unified.tsv',minimum_detection_rate=0.02):

  # read count files
  logging.info('Reading %d count files'%(len(count_files)))
  frames = [pd.read_table(filename,header=0,index_col=0) for filename in count_files]
  
  # construct the multi index
  for filename,frame in  zip(count_files,frames):
  	frame.columns = pd.MultiIndex.from_product([[filename],frame.columns],names=['Sample','Coordinate'])
  	frame.index.name = 'Gene'
  
  # concatenate counts
  result = pd.concat(frames,copy=False,axis=1,sort=True)
  logging.info('We have detected %d genes'%(result.shape[0]))
  # fill NaNs with zeros
  result = result.fillna(0).astype(int)
  
  # discard lowly expressed genes
  result = result[((result > 0).sum(axis=1)/float(result.shape[1])) > minimum_detection_rate]
  logging.info('We keep %d genes after discarding the lowly expressed genes (detected in less than %.2f%% of the ST spots)'%(result.shape[0],100.0*minimum_detection_rate))

  # print the median sequencing depth
  logging.info('The median sequencing depth across the ST spots is %d'%(numpy.median(result.sum(0))))
  
  # write the modified count files back to the disk
  for filename in result.columns.levels[0]:
  	result[filename].to_csv(filename+suffix,sep='\t',index=True)

if __name__ == '__main__':

  parser = argparse.ArgumentParser(
    description='A script for preparing count files for Splotch')
  parser.add_argument('-c','--count_files',action='store',
                      dest='count_files',type=str,nargs='+',
                      required=True,help='list of read count filenames')
  parser.add_argument('-s','--suffix',action='store',
                      dest='suffix',type=str,required=False,
                      default='.unified.tsv',
                      help='suffix to be added to the output filenames (default is .unified.tsv)')
  parser.add_argument('-d','--minimum_detection_rate',action='store',
                      dest='minimum_detection_rate',type=float,required=False,
                      default=0.02,help='minimum detection rate (default is 0.02)')
  parser.add_argument('-v','--version',action='version',
                      version='%s %s'%(parser.prog,splotch.__version__))

  options = parser.parse_args()

  # check that the supplied read count files exist
  for filename in options.count_files:
    if not os.path.isfile(filename):
      logging.critical('Count file %s does not exist!'%(filename))
      sys.exit(1)

  # check that the minimum detection rate is valid
  if options.minimum_detection_rate < 0 or options.minimum_detection_rate > 1:
    logging.critical('Minimum detection rate should be between 0 and 1!')
    sys.exit(1)

  prepare_count_files(options.count_files,options.suffix,options.minimum_detection_rate)

  sys.exit(0)
